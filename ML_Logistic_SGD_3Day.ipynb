{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 로지스틱 회귀 주요 하이퍼파라미터 상세 설명\n",
        "\n",
        "---\n",
        "\n",
        "### 1. C (정규화 강도, Inverse of regularization strength)\n",
        "\n",
        "- **정의**: 모델의 규제(패널티) 세기를 조절하는 반비례 상수  \n",
        "  \\[\n",
        "    \\min \\; \\frac{1}{n}\\sum L(y, \\hat y) \\;+\\;\\frac{1}{2C}\\|w\\|^2\n",
        "  \\]\n",
        "- **C 값이 작을수록**  \n",
        "  - \\(1/2C\\) 패널티 항이 커져 **강한 규제**  \n",
        "  - 계수(w)가 작아지고, 모델이 단순해져 **과소적합** 경향  \n",
        "- **C 값이 클수록**  \n",
        "  - 규제 약화 → 계수가 자유로워져 **과적합** 가능  \n",
        "- **사용 팁**:  \n",
        "  - 데이터가 **노이즈가 많거나** 특성 간 상관이 높으면 작은 C(강한 규제)  \n",
        "  - 데이터가 **매우 깔끔하고** 특성 수가 적으면 큰 C 혹은 규제 없음(‘none’)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. penalty (규제 유형)\n",
        "\n",
        "- **‘l2’ (Ridge 유형)**  \n",
        "  - 계수 제곱합을 패널티로 사용  \n",
        "  - 모든 특성을 조금씩 중요도로 남기면서 과적합 억제  \n",
        "- **‘l1’ (Lasso 유형)**  \n",
        "  - 계수 절댓값 합을 패널티로 사용  \n",
        "  - 일부 계수를 정확히 **0**으로 만들어 **특성 선택**  \n",
        "- **‘elasticnet’** (scikit-learn 지원 시)  \n",
        "  - L₁과 L₂를 혼합 (`l1_ratio` 추가 지정)  \n",
        "- **‘none’**  \n",
        "  - 규제를 적용하지 않음 (OLS 형태)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. solver (최적화 알고리즘)\n",
        "\n",
        "- **‘liblinear’**  \n",
        "  - 소규모 데이터, L₁ 규제 지원  \n",
        "  - 1대1(one-vs-rest) 방식으로 다중 클래스 해석  \n",
        "- **‘lbfgs’**  \n",
        "  - 준뉴턴 방법(Quasi-Newton)  \n",
        "  - L₂ 규제만 지원, 대규모 데이터/다중 클래스에 적합  \n",
        "- **‘saga’**  \n",
        "  - 확률적 경사하강 기반 변형  \n",
        "  - L₁, L₂, ElasticNet 모두 지원  \n",
        "  - 대규모·희소 데이터에 강함  \n",
        "- **‘newton-cg’** / **‘sag’** 등  \n",
        "  - 각각 수치 특성에 맞춰 속도/메모리 효율 차이 있음\n",
        "\n",
        "---\n",
        "\n",
        "### 4. multi_class (다중 클래스 처리 방식)\n",
        "\n",
        "- **‘ovr’ (One-vs-Rest)**  \n",
        "  - 각 클래스를 양성-음성 문제로 분리하여 독립 학습  \n",
        "  - 클래스 수 만큼 모델 생성  \n",
        "- **‘multinomial’**  \n",
        "  - 소프트맥스 함수를 활용해 다중 클래스 동시 최적화  \n",
        "  - 로짓벡터 전체를 한 번에 학습 → 확률 분포 일관성 보장  \n",
        "- **팁**: 클래스 수가 많지 않고 데이터가 균형적이면 `multinomial`,  \n",
        "  불균형하거나 희소한 경우 OVR이 더 안정적일 수 있음.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. max_iter (최대 반복 횟수)\n",
        "\n",
        "- 최적화 수렴을 위해 solver가 반복할 최대 이터레이션 수  \n",
        "- 너무 작으면 “수렴 실패” 경고 → 늘려야 함 (`max_iter=1000` 등)\n",
        "\n",
        "---\n",
        "\n",
        "### 6. tol (수렴 허용 오차)\n",
        "\n",
        "- 반복 중 “손실 변화량”이 이 값 이하로 떨어지면 조기 종료  \n",
        "- 작은 tol → 엄격한 수렴 (반복 많아짐), 큰 tol → 빠른 종료\n",
        "\n",
        "---\n",
        "\n",
        "> **실전 팁**  \n",
        "> - `GridSearchCV`로 **C**, **penalty**, **solver** 조합을 탐색  \n",
        "> - `saga` + `l1` (Lasso) → 자동 특성 선택  \n",
        "> - `lbfgs` + `multinomial` → 다중 클래스에 부드러운 확률 예측  \n",
        "> - `liblinear` + `l2` → 소규모·이진 분류에서 빠르고 안정적  \n"
      ],
      "metadata": {
        "id": "KODQvxI2cykR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh2JUMwncsek"
      },
      "outputs": [],
      "source": [
        "**코드 셀 (Python)**\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# (예시) Iris 데이터셋 사용\n",
        "from sklearn.datasets import load_iris\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 1) Pipeline 정의\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('clf', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# 2) 하이퍼파라미터 그리드\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10],\n",
        "    'clf__penalty': ['l2', 'l1'],\n",
        "    'clf__solver': ['liblinear', 'saga'],\n",
        "    'clf__multi_class': ['ovr', 'multinomial']\n",
        "}\n",
        "\n",
        "# 3) GridSearchCV 설정\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "gs = GridSearchCV(\n",
        "    pipe,\n",
        "    param_grid,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 4) 학습 및 결과 확인\n",
        "gs.fit(X_tr, y_tr)\n",
        "print(\"Best parameters:\", gs.best_params_)\n",
        "print(\"Best cross-val accuracy:\", gs.best_score_)\n",
        "\n",
        "# 5) 테스트 세트 성능\n",
        "best_model = gs.best_estimator_\n",
        "y_pred = best_model.predict(X_te)\n",
        "print(\"Test set accuracy:\", accuracy_score(y_te, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGD (Stochastic Gradient Descent) 사용 시기와 손실 함수 심화\n",
        "\n",
        "---\n",
        "\n",
        "## 1. SGD란 무엇인가?\n",
        "\n",
        "- **경사하강법**(Gradient Descent)의 변형으로,  \n",
        "  전체 데이터가 아니라 **미니배치**(또는 한 샘플) 단위로 기울기를 계산해 파라미터를 업데이트  \n",
        "- 매 스텝마다 데이터의 일부만 보기 때문에  \n",
        "  - **계산 비용**이 작고 메모리 효율적  \n",
        "  - **노이즈**가 섞여 있어 지역 최적(local minima) 회피에 도움  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. 언제 SGD를 사용하나?\n",
        "\n",
        "1. **대규모 데이터셋**  \n",
        "   - 전체 데이터를 한 번에 메모리에 올리기 힘들 때  \n",
        "   - 배치 크기(batch size)를 조절해 GPU/CPU 자원 활용\n",
        "\n",
        "2. **온라인 학습(Online Learning)**  \n",
        "   - 새로운 데이터가 지속적으로 유입될 때  \n",
        "   - 한 샘플씩 즉시 업데이트하며 모델을 지속 개선\n",
        "\n",
        "3. **딥러닝/신경망**  \n",
        "   - 모델 파라미터(가중치)가 수십만~수백만 개일 때  \n",
        "   - 전체 그래디언트를 구하기엔 계산 비용이 너무 큼\n",
        "\n",
        "4. **빠른 프로토타이핑**  \n",
        "   - 빠른 수렴 경향을 이용해 초반 성능을 빠르게 확인\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 손실 함수(Loss Function) 개념\n",
        "\n",
        "- **목적**: 모델 예측  \\(\\hat y\\)와 실제값 \\(y\\) 간 차이를 수치화  \n",
        "- **최적화 목표**:  \n",
        "  \\[\n",
        "    \\min_\\theta \\; L(\\theta)\n",
        "  \\]\n",
        "  여기서 \\(L\\)이 손실 함수, \\(\\theta\\)는 파라미터(가중치, 절편 등)\n",
        "\n",
        "---\n",
        "\n",
        "## 4. 대표적 손실 함수\n",
        "\n",
        "| 문제 유형      | 손실 함수               |\n",
        "| **회귀**      | MSE (Mean Squared Error)|\n",
        "|             | MAE (Mean Absolute Error)|\n",
        "| **이진 분류**  | Binary Cross-Entropy|\n",
        "| **다중 분류**  | Categorical Cross-Entropy  |\n",
        "| **히브리드**  | Huber Loss                 |\n",
        "\n",
        "---\n",
        "\n",
        "## 5. 손실 함수 선택 기준\n",
        "\n",
        "1. **오차 특성**  \n",
        "   - 이상치가 많으면 MAE/Huber  \n",
        "   - 극단값 중점을 두면 MSE\n",
        "\n",
        "2. **출력 형태**  \n",
        "   - 확률 예측이 필요하면 Cross-Entropy\n",
        "\n",
        "3. **연속 vs 범주형**  \n",
        "   - 회귀: MSE/MAE  \n",
        "   - 분류: Cross-Entropy\n",
        "\n",
        "4. **수치 안정성**  \n",
        "   - 로그 항이 들어가는 경우 `ε` 추가(클리핑, log-softmax)\n",
        "\n",
        "---\n",
        "\n",
        "> **요약**:  \n",
        "> - **SGD**는 대용량·온라인·딥러닝에 특화된 최적화 기법  \n",
        "> - **손실 함수**는 문제 유형과 데이터 특성에 맞춰 선택  \n",
        "> - SGD와 손실 함수(미니배치) 조합으로 효율적이고 안정적인 학습이 가능  \n"
      ],
      "metadata": {
        "id": "qbKeMy1odFtf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qvh4-zP-fE5r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}